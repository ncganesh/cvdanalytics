{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# Import libraries\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import preprocessor as p\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(messages_filepath, categories_filepath):\n",
    "\n",
    "    messages = pd.read_csv(messages_filepath)\n",
    "    categories = pd.read_csv(categories_filepath)\n",
    "    df = pd.merge(messages, categories, on='id', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "\n",
    "    # Expand categories into separate columns\n",
    "    categories = df.categories.str.split(';', expand=True)\n",
    "    colnames = categories.iloc[0].str.split('-', expand=True)[0].tolist()\n",
    "    categories.columns = colnames\n",
    "    \n",
    "    # Clean values and convert to numeric if the category is not constant\n",
    "    for column in categories.columns:\n",
    "        if categories[column].nunique() > 1:\n",
    "            categories[column] = categories[column].apply(lambda r: r[-1]).astype(int)\n",
    "        else:\n",
    "            categories.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "    # Combine original df and expanded categories\n",
    "    return pd.concat([df.drop('categories', axis=1), categories], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_filepath = 'data/disaster_messages.csv'\n",
    "categories_filepath = 'data/disaster_categories.csv'\n",
    "df = load_data(messages_filepath, categories_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n"
     ]
    }
   ],
   "source": [
    "print('Cleaning data...')\n",
    "cleaneddf = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
