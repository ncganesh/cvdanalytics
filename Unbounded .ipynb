{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# Import libraries\n",
    "import joblib\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import preprocessor as p\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "import re\n",
    "from flask import Flask\n",
    "from flask import json\n",
    "from flask import request\n",
    "from datetime import datetime,timedelta\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(messages_filepath, categories_filepath):\n",
    "\n",
    "    messages = pd.read_csv(messages_filepath)\n",
    "    categories = pd.read_csv(categories_filepath)\n",
    "    df = pd.merge(messages, categories, on='id', how='left')\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "\n",
    "    # Expand categories into separate columns\n",
    "    categories = df.categories.str.split(';', expand=True)\n",
    "    colnames = categories.iloc[0].str.split('-', expand=True)[0].tolist()\n",
    "    categories.columns = colnames\n",
    "    \n",
    "    # Clean values and convert to numeric if the category is not constant\n",
    "    for column in categories.columns:\n",
    "        if categories[column].nunique() > 1:\n",
    "            categories[column] = categories[column].apply(lambda r: r[-1]).astype(int)\n",
    "        else:\n",
    "            categories.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "    # Combine original df and expanded categories\n",
    "    return pd.concat([df.drop('categories', axis=1), categories], axis=1).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_filepath = 'data/disaster_messages.csv'\n",
    "categories_filepath = 'data/disaster_categories.csv'\n",
    "df = load_data(messages_filepath, categories_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning data...')\n",
    "cleaneddf = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleaneddf['message'].copy()\n",
    "Y = cleaneddf.iloc[:, 4:].copy()\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(LogisticRegression(max_iter=1000, random_state=0)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid to search\n",
    "parameters = {\n",
    "    'tfidf__max_df': [0.01, 0.1, 0.2],\n",
    "    'tfidf__max_features': [None, 1000, 10000],\n",
    "    'tfidf__ngram_range': [(1, 1), (2, 2), (3, 3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipeline, parameters, cv=4, n_jobs=12, verbose=2)\n",
    "gs.fit(X, Y)\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model_14may.sav'\n",
    "pickle.dump(logreg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['related', 'request', 'offer', 'aid_related', \n",
    "'medical_help', 'medical_products',\n",
    "'search_and_rescue', 'security', 'military', \n",
    "'child_alone', 'water', 'food', 'shelter', \n",
    "'clothing', 'money', 'missing_people', 'refugees', \n",
    "'death', 'other_aid', 'infrastructure_related', \n",
    "'transport', 'buildings', 'electricity', 'tools', \n",
    "'hospitals', 'shops', 'aid_centers', \n",
    "'other_infrastructure', 'weather_related', \n",
    "'floods', 'storm', 'fire', 'earthquake', 'cold', \n",
    "'other_weather', 'direct_report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = loaded_model.predict_proba([\"comes to the rescue of a COVID-19 positive patient\"])\n",
    "pred_prob = np.argmax([row[0][1] for row in pred_probs])\n",
    "labels[pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = loaded_model.predict_proba([\": We need Ur help\"])\n",
    "pred_prob = np.argmax([row[0][1] for row in pred_probs])\n",
    "labels[pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = loaded_model.predict_proba([\"Need Blood and Need plasma From Covid recovered patientAt\"])\n",
    "pred_prob = np.argmax([row[0][1] for row in pred_probs])\n",
    "labels[pred_prob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------ APIS ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from flask import Flask\n",
    "from flask import json\n",
    "from flask import request\n",
    "from datetime import datetime,timedelta\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5009/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [14/May/2021 13:53:15] \"\u001b[37mPOST /get_last_1mindata HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from flask import Flask\n",
    "from flask import json\n",
    "from flask import request\n",
    "from datetime import datetime,timedelta\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch(hosts=[\"http://3.238.229.207:9200/\"])\n",
    "app = Flask(__name__)\n",
    "\n",
    "filename = 'finalized_model_14may.sav'\n",
    "\n",
    "labels = ['related', 'request', 'offer', 'aid_related', \n",
    "'medical_help', 'medical_products',\n",
    "'search_and_rescue', 'security', 'military', \n",
    "'child_alone', 'water', 'food', 'shelter', \n",
    "'clothing', 'money', 'missing_people', 'refugees', \n",
    "'death', 'other_aid', 'infrastructure_related', \n",
    "'transport', 'buildings', 'electricity', 'tools', \n",
    "'hospitals', 'shops', 'aid_centers', \n",
    "'other_infrastructure', 'weather_related', \n",
    "'floods', 'storm', 'fire', 'earthquake', 'cold', \n",
    "'other_weather', 'direct_report']\n",
    "\n",
    "#tweet = content['text']\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "@app.route('/get_intent_predictions', methods = ['POST'])\n",
    "\n",
    "def get_prob():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        tweet = request.form['tweettext']\n",
    "        #print(tweet)\n",
    "        #predictions = loaded_model.predict([tweet])\n",
    "        #result = np.where(predictions == 1)\n",
    "        #print(\"result\",result)\n",
    "        #preds = [labels[xi] for xi in result[1]]\n",
    "        pred_probs = loaded_model.predict_proba([cleanedtweet])\n",
    "        pred_prob = np.argmax([row[0][1] for row in pred_probs])\n",
    "        preds = labels[pred_prob]\n",
    "        print(\"PREDS-------\",preds)\n",
    "    except Exception as e:\n",
    "        print(\"Exception\",e)\n",
    "        preds = \"related\"\n",
    "        \n",
    "    \n",
    "    return str(preds)\n",
    "\n",
    "\n",
    "@app.route('/get_last_1mindata', methods = ['POST'])\n",
    "def get_lastmindata():\n",
    "    result = es.search(index=\"twitter_india_covid\", body={\n",
    "        \"query\": {\n",
    "            \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                    \"gte\": datetime.utcnow() - timedelta(minutes=15),\n",
    "                    \"lt\": datetime.utcnow()\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        # ensure that we return all docs in our test corpus\n",
    "       \n",
    "    })\n",
    "    lis = []\n",
    "    for item in result['hits']['hits']:\n",
    "        lis.append(item['_source'])\n",
    "    \n",
    "    responses = pd.DataFrame(lis).to_json(orient=\"records\")\n",
    "    return responses\n",
    "\n",
    "@app.route('/get_fulldata', methods = ['POST'])\n",
    "def get_fulldata():\n",
    "    s = Search(using=es, index=\"twitter_india_covid\")\n",
    "    df = pd.DataFrame([hit.to_dict() for hit in s.scan()])\n",
    "    responses = df.to_json(orient=\"records\")\n",
    "    return responses             \n",
    "                      \n",
    "\n",
    "@app.route('/semantic_search', methods = ['POST'])\n",
    "def search():\n",
    "    queries = str(request.form['userquery'])\n",
    "    query = {\n",
    "        \"size\": 30,\n",
    "        \"query\": {\n",
    "            \"query_string\": {\"query\": queries}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    for result in es.search(index=\"twitter_india_covid\", body=query)[\"hits\"][\"hits\"]:\n",
    "        source = result[\"_source\"]\n",
    "        print(source)\n",
    "        results.append((min(result[\"_score\"], 18) / 18, source[\"text\"]))\n",
    "        \n",
    "    #similarity = Similarity(\"valhalla/distilbart-mnli-12-3\")\n",
    "    #results = [text for _, text in search(query, limit * 10)]\n",
    "    #return [(score, results[x]) for x, score in similarity(query, results)][:limit]\n",
    "    \n",
    "    responses = pd.DataFrame(results,columns = ['results','Text']).to_json(orient=\"records\")\n",
    "\n",
    "    return responses\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5009)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
